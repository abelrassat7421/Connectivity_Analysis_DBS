{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "from nilearn import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "\n",
    "from graphUtils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Loading Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Atlas of choice has 116 labels.\n"
     ]
    }
   ],
   "source": [
    "atlas = datasets.fetch_atlas_aal(data_dir=\"/ptmp/arassat/nilearn_data\")\n",
    "atlas_filename = atlas.maps\n",
    "labels = atlas.labels\n",
    "num_atlas_reg = len(labels)\n",
    "print(\"The Atlas of choice has {} labels.\".format(num_atlas_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Loading connectivity matrices to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 116, 104)\n"
     ]
    }
   ],
   "source": [
    "num_subjects_tot = 115\n",
    "\n",
    "subjects_removed = [6, 13, 45, 64, 78, 80, 88, 98, 105, 108, 113]\n",
    "subjects = [i for i in range(1, num_subjects_tot + 1)]\n",
    "\n",
    "for i in subjects_removed:\n",
    "    subjects.remove(i)\n",
    "\n",
    "num_subjects = num_subjects_tot - len(subjects_removed)\n",
    "\n",
    "data_OFF = np.zeros((num_atlas_reg, num_atlas_reg, num_subjects))\n",
    "data_ON = np.zeros((num_atlas_reg, num_atlas_reg, num_subjects))\n",
    "print(data_OFF.shape)\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    if subject < 10:\n",
    "        file_str_OFF = \"/ptmp/arassat/Abel/connectivity_matrices_aal/00%d_OFF_aal_connectivity.csv\" % (subject)\n",
    "        file_str_ON = \"/ptmp/arassat/Abel/connectivity_matrices_aal/00%d_ON_aal_connectivity.csv\" % (subject)\n",
    "        data_OFF[:, :, i-1] = np.genfromtxt(file_str_OFF,delimiter=',')[1:,1:]\n",
    "        data_ON[:, :, i-1] = np.genfromtxt(file_str_ON,delimiter=',')[1:,1:]\n",
    "    elif subject >= 10 and subject < 100:\n",
    "        file_str_OFF = \"/ptmp/arassat/Abel/connectivity_matrices_aal/0%d_OFF_aal_connectivity.csv\" % (subject)\n",
    "        file_str_ON = \"/ptmp/arassat/Abel/connectivity_matrices_aal/0%d_ON_aal_connectivity.csv\" % (subject)\n",
    "        data_OFF[:, :, i-1] = np.genfromtxt(file_str_OFF,delimiter=',')[1:,1:]\n",
    "        data_ON[:, :, i-1] = np.genfromtxt(file_str_ON,delimiter=',')[1:,1:]\n",
    "    else:\n",
    "        file_str_OFF = \"/ptmp/arassat/Abel/connectivity_matrices_aal/%d_OFF_aal_connectivity.csv\" % (subject)\n",
    "        file_str_ON = \"/ptmp/arassat/Abel/connectivity_matrices_aal/%d_ON_aal_connectivity.csv\" % (subject)\n",
    "        data_OFF[:, :, i-1] = np.genfromtxt(file_str_OFF,delimiter=',')[1:,1:]\n",
    "        data_ON[:, :, i-1] = np.genfromtxt(file_str_ON,delimiter=',')[1:,1:]\n",
    "    \n",
    "    np.fill_diagonal(data_OFF[:,:,i-1],0)\n",
    "    np.fill_diagonal(data_ON[:,:,i-1],0)\n",
    "\n",
    "    # removing the negative correlation values\n",
    "    data_OFF[:,:,i-1] = np.where(data_OFF[:,:,i-1] > 0, data_OFF[:,:,i-1], 0)\n",
    "    data_ON[:,:,i-1] = np.where(data_ON[:,:,i-1] > 0, data_ON[:,:,i-1], 0)\n",
    "\n",
    "data = {\n",
    "    \"Off\" : data_OFF,\n",
    "    \"On\"  : data_ON\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3. Loading thresholded & null-hyposthesis networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bct\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting the proportional thresholds\n",
    "# In T. Xu et al. thresholds = np.arange(0.1, 0.55, 0.01)\n",
    "thresholds = np.arange(0.05, 0.5, 0.01)\n",
    "thresholds_wei_test = np.arange(0.05, 0.50, 0.05)\n",
    "num_thresholds = thresholds.size\n",
    "num_thresholds_wei_test = thresholds_wei_test.size\n",
    "# random seed for generating null-hypothesis networks\n",
    "seed = 42\n",
    "\n",
    "# network will be saved in dictionaries\n",
    "thresholded_data = {\n",
    "    \"Off\" : np.zeros((num_atlas_reg, num_atlas_reg, num_subjects, num_thresholds)),\n",
    "    \"On\"  : np.zeros((num_atlas_reg, num_atlas_reg, num_subjects, num_thresholds))\n",
    "}\n",
    "null_data = {\n",
    "    \"Off\"  :  np.zeros(thresholded_data[\"Off\"].shape),\n",
    "    \"On\"   :  np.zeros(thresholded_data[\"On\"].shape)\n",
    "}\n",
    "\"\"\"\n",
    "thresholded_data_dist = {\n",
    "    \"Off\" : np.zeros((num_atlas_reg, num_atlas_reg, num_subjects, num_thresholds_wei_test)),\n",
    "    \"On\"  : np.zeros((num_atlas_reg, num_atlas_reg, num_subjects, num_thresholds_wei_test))\n",
    "}\n",
    "null_data_dist = {\n",
    "    \"Off\"  :  np.zeros(thresholded_data_dist[\"Off\"].shape),\n",
    "    \"On\"   :  np.zeros(thresholded_data_dist[\"On\"].shape)\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nth_data = np.load(\\'thresholded_data.npz\\')\\nnull_data_Off = np.load(\\'null_data_Off.npz\\')\\nnull_data_On = np.load(\\'null_data_On.npz\\')\\nthresholded_data[\"Off\"] = thresholded_data[\\'thresholded_data_Off\\']\\nthresholded_data[\"On\"] = thresholded_data[\\'thresholded_data_On\\']\\nnull_data[\"Off\"] = null_data_Off[\\'null_data_Off\\']\\nnull_data[\"On\"] = null_data_On[\\'null_data_On\\']\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarized networks\n",
    "data = np.load('thresholded_and_null_data.npz')\n",
    "thresholded_data[\"Off\"] = data['thresholded_data_Off']\n",
    "thresholded_data[\"On\"] = data['thresholded_data_On']\n",
    "null_data[\"Off\"] = data['null_data_Off']\n",
    "null_data[\"On\"] = data['null_data_On']\n",
    "\n",
    "\"\"\"\n",
    "# Weighted networks\n",
    "data_dist = np.load('thresholded_data_dist.npz')\n",
    "thresholded_data_dist[\"Off\"] = data_dist[\"thresholded_data_Off_dist\"]\n",
    "thresholded_data_dist[\"On\"] = data_dist[\"thresholded_data_On_dist\"]\n",
    "null_data_dist['Off'] = np.load('null_data_Off_dist.npz')['null_data_Off_dist']\n",
    "null_data_dist['On']  = np.load('null_data_On_dist.npz')['null_data_On_dist']\n",
    "\"\"\"\n",
    "\n",
    "# when networks generated separately due to high wall time for generation (dense thresholding) \n",
    "\"\"\"\n",
    "th_data = np.load('thresholded_data.npz')\n",
    "null_data_Off = np.load('null_data_Off.npz')\n",
    "null_data_On = np.load('null_data_On.npz')\n",
    "thresholded_data[\"Off\"] = thresholded_data['thresholded_data_Off']\n",
    "thresholded_data[\"On\"] = thresholded_data['thresholded_data_On']\n",
    "null_data[\"Off\"] = null_data_Off['null_data_Off']\n",
    "null_data[\"On\"] = null_data_On['null_data_On']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 116, 104, 45) (116, 116, 104, 45) (116, 116, 104, 9) (116, 116, 104, 9)\n"
     ]
    }
   ],
   "source": [
    "print(thresholded_data[\"Off\"].shape, null_data[\"Off\"].shape, thresholded_data_dist[\"Off\"].shape, null_data_dist['Off'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing Network Measures from T. Xu et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_measures_Off = {\n",
    "    \"norm_charact_path_length\"  : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"norm_clustering_coef\"      : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"small_worldness\"           : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"norm_global_eff\"           : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"norm_local_eff\"            : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"size_of_LCC\"               : np.zeros((num_thresholds, num_subjects))\n",
    "}\n",
    "\n",
    "network_measures_On = {\n",
    "    \"norm_charact_path_length\"  : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"norm_clustering_coef\"      : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"small_worldness\"           : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"norm_global_eff\"           : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"norm_local_eff\"            : np.zeros((num_thresholds, num_subjects)),\n",
    "    \"size_of_LCC\"               : np.zeros((num_thresholds, num_subjects))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network_measures_wei_Off = {\n",
    "    \"norm_charact_path_length\"  : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"norm_clustering_coef\"      : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"small_worldness\"           : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"norm_global_eff\"           : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"norm_local_eff\"            : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"size_of_LCC\"               : np.zeros((num_thresholds_wei_test, num_subjects))\n",
    "}\n",
    "\n",
    "network_measures_wei_On = {\n",
    "    \"norm_charact_path_length\"  : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"norm_clustering_coef\"      : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"small_worldness\"           : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"norm_global_eff\"           : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"norm_local_eff\"            : np.zeros((num_thresholds_wei_test, num_subjects)),\n",
    "    \"size_of_LCC\"               : np.zeros((num_thresholds_wei_test, num_subjects))\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_measures_Off = {\n",
    "    \"clustering_coef\"           : np.zeros((num_thresholds, num_subjects, num_atlas_reg)),\n",
    "    \"local_eff\"                 : np.zeros((num_thresholds, num_subjects, num_atlas_reg)), \n",
    "    \"degree\"                    : np.zeros((num_thresholds, num_subjects, num_atlas_reg))\n",
    "}\n",
    "                                           \n",
    "centrality_measures_On = {\n",
    "    \"clustering_coef\"           : np.zeros((num_thresholds, num_subjects, num_atlas_reg)),\n",
    "    \"local_eff\"                 : np.zeros((num_thresholds, num_subjects, num_atlas_reg)),\n",
    "    \"degree\"                    : np.zeros((num_thresholds, num_subjects, num_atlas_reg))\n",
    "}                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "centrality_measures_wei_Off = {\n",
    "    \"clustering_coef\"           : np.zeros((num_thresholds_wei_test, num_subjects, num_atlas_reg)),\n",
    "    \"local_eff\"                 : np.zeros((num_thresholds_wei_test, num_subjects, num_atlas_reg)), \n",
    "    \"degree\"                    : np.zeros((num_thresholds_wei_test, num_subjects, num_atlas_reg)) # i.e., strength of each node / region\n",
    "}\n",
    "                                           \n",
    "centrality_measures_wei_On = {\n",
    "    \"clustering_coef\"           : np.zeros((num_thresholds_wei_test, num_subjects, num_atlas_reg)),\n",
    "    \"local_eff\"                 : np.zeros((num_thresholds_wei_test, num_subjects, num_atlas_reg)),\n",
    "    \"degree\"                    : np.zeros((num_thresholds_wei_test, num_subjects, num_atlas_reg)) # i.e., strength of each node / region\n",
    "}  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Binarized Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55097c9083ea4811a7d7969f9b4b2160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in tqdm(range(num_thresholds)):     \n",
    "    # normalized characteristic path and global efficiency\n",
    "    charpath_network_Off, global_eff_network_Off = charpath_and_global_eff(thresholded_data[\"Off\"][:, :, :, j], num_subjects)\n",
    "    charpath_null_Off, global_eff_null_Off = charpath_and_global_eff(null_data[\"Off\"][:, :, :, j], num_subjects)\n",
    "    charpath_network_On, global_eff_network_On = charpath_and_global_eff(thresholded_data[\"On\"][:, :, :, j], num_subjects)\n",
    "    charpath_null_On, global_eff_null_On = charpath_and_global_eff(null_data[\"On\"][:, :, :, j], num_subjects)\n",
    "\n",
    "    network_measures_Off[\"norm_charact_path_length\"][j][:] = charpath_network_Off / charpath_null_Off\n",
    "    network_measures_On[\"norm_charact_path_length\"][j][:] = charpath_network_On / charpath_null_On\n",
    "    network_measures_Off[\"norm_global_eff\"][j][:] = global_eff_network_Off / global_eff_null_Off\n",
    "    network_measures_On[\"norm_global_eff\"][j][:]= global_eff_network_On / global_eff_null_On\n",
    "    \n",
    "    # normalized clustering coefficient\n",
    "    network_clustering_coef_Off = np.array([np.mean(bct.clustering_coef_bu(thresholded_data[\"Off\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    null_clustering_coef_Off = np.array([np.mean(bct.clustering_coef_bu(null_data[\"Off\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    network_clustering_coef_On = np.array([np.mean(bct.clustering_coef_bu(thresholded_data[\"On\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    null_clustering_coef_On = np.array([np.mean(bct.clustering_coef_bu(null_data[\"On\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    network_measures_Off[\"norm_clustering_coef\"][j][:] = network_clustering_coef_Off / null_clustering_coef_Off\n",
    "    network_measures_On[\"norm_clustering_coef\"][j][:] = network_clustering_coef_On / null_clustering_coef_On\n",
    "\n",
    "    # small-worldness\n",
    "    network_measures_Off[\"small_worldness\"][j][:] = np.array(network_measures_Off[\"norm_clustering_coef\"][j]) / np.array(network_measures_Off[\"norm_charact_path_length\"][j])\n",
    "    network_measures_On[\"small_worldness\"][j][:] = np.array(network_measures_On[\"norm_clustering_coef\"][j]) / np.array(network_measures_On[\"norm_charact_path_length\"][j]) \n",
    "    \n",
    "    # Size of the Largest Connected Component (LCC)\n",
    "    network_measures_Off[\"size_of_LCC\"][j][:] = np.array([len(max(nx.connected_components(nx.from_numpy_array(thresholded_data[\"Off\"][:, :, i, j])))) for i in range(num_subjects)])\n",
    "    network_measures_On[\"size_of_LCC\"][j][:] = np.array([len(max(nx.connected_components(nx.from_numpy_array(thresholded_data[\"On\"][:, :, i, j])))) for i in range(num_subjects)])\n",
    "    \n",
    "    # Clustering coefficient saved for all nodes\n",
    "    centrality_net_clust_Off = np.array([bct.clustering_coef_bu(thresholded_data[\"Off\"][:, :, i, j]) for i in range(num_subjects)])\n",
    "    centrality_net_clust_On = np.array([bct.clustering_coef_bu(thresholded_data[\"On\"][:, :, i, j]) for i in range(num_subjects)])\n",
    "    # circumventing the issue of having a 0 clustering coefficient for a given region by dividing by the mean clustering coefficent (over all regions) \n",
    "    # of the null-hypothesis graph for a given graph density and subject. \n",
    "    centrality_measures_Off[\"clustering_coef\"][j][:][:] = centrality_net_clust_Off / null_clustering_coef_Off[:, np.newaxis]\n",
    "    centrality_measures_On[\"clustering_coef\"][j][:][:] = centrality_net_clust_On / null_clustering_coef_On[:, np.newaxis]\n",
    "    \n",
    "    # Degree for all nodes\n",
    "    centrality_net_deg_Off = np.array([bct.degrees_und(thresholded_data[\"Off\"][:, :, i, j]) for i in range(num_subjects)])\n",
    "    centrality_net_deg_On = np.array([bct.degrees_und(thresholded_data[\"On\"][:, :, i, j]) for i in range(num_subjects)])\n",
    "    null_mean_deg_Off = np.array([np.mean(bct.degrees_und(null_data[\"Off\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    null_mean_deg_On = np.array([np.mean(bct.degrees_und(null_data[\"On\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    centrality_measures_Off[\"degree\"][j][:][:] = centrality_net_deg_Off / null_mean_deg_Off[:, np.newaxis]\n",
    "    centrality_measures_On[\"degree\"][j][:][:] = centrality_net_deg_On / null_mean_deg_On[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(104,) (104,)\n"
     ]
    }
   ],
   "source": [
    "print(type(centrality_measures_Off[\"clustering_coef\"][0]))\n",
    "print(type(network_measures_Off[\"small_worldness\"][0]))\n",
    "print(type(network_measures_Off[\"norm_clustering_coef\"][j]), type(network_measures_Off[\"norm_charact_path_length\"][j]))\n",
    "print(type(network_clustering_coef_On), type(null_clustering_coef_On))\n",
    "print(network_clustering_coef_On.shape, null_clustering_coef_On.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36784f9f00214c14aee5f3506d8d74bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "local_eff_network_Off = np.array([local_eff(thresholded_data[\"Off\"][:, :, :, j], num_subjects) for j in tqdm(range(num_thresholds))])\n",
    "# Save the array to a text file\n",
    "np.savetxt('local_eff_network_Off.txt', local_eff_network_Off)\n",
    "# local_eff_network_Off = np.loadtxt('local_eff_network_Off.txt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce436eb5c9314afb86998c1083edf4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_eff_network_On = np.array([local_eff(thresholded_data[\"On\"][:, :, :, j], num_subjects) for j in tqdm(range(num_thresholds))])\n",
    "np.savetxt('local_eff_network_On.txt', local_eff_network_On)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71be276ef9644d5943941482aead798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4906/3439679833.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  network_measures_Off[\"norm_local_eff\"][j][:] = local_eff_network_Off / local_eff_null_Off\n",
      "/tmp/ipykernel_4906/3439679833.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  network_measures_On[\"norm_local_eff\"][j][:] = local_eff_network_On / local_eff_null_On\n"
     ]
    }
   ],
   "source": [
    "# Modify if the lists above can be saved correctly\n",
    "\n",
    "local_eff_network_Off = np.loadtxt('local_eff_network_Off.txt')\n",
    "local_eff_network_On = np.loadtxt('local_eff_network_On.txt')\n",
    "\n",
    "for j in tqdm(range(num_thresholds)): \n",
    "    local_eff_null_Off = local_eff(null_data[\"Off\"][:, :, :, j], num_subjects)\n",
    "    local_eff_null_On = local_eff(null_data[\"On\"][:, :, :, j], num_subjects)\n",
    "    network_measures_Off[\"norm_local_eff\"][j][:] = local_eff_network_Off[j] / local_eff_null_Off\n",
    "    network_measures_On[\"norm_local_eff\"][j][:] = local_eff_network_On[j] / local_eff_null_On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c924b2b94b645798cca56cb60c5fe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4906/4001460178.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  centrality_measures_Off[\"local_eff\"][j][:][:] = centrality_net_local_eff_Off / local_eff_null_Off[:, np.newaxis]\n",
      "/tmp/ipykernel_4906/4001460178.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  centrality_measures_On[\"local_eff\"][j][:][:] = centrality_net_local_eff_On / local_eff_null_On[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "for j in tqdm(range(num_thresholds)): \n",
    "    # Local efficiency for all nodes (binarized networks)\n",
    "    centrality_net_local_eff_Off = np.array([[local_efficiency_node(nx.from_numpy_array(thresholded_data[\"Off\"][:, :, i, j]), node) \n",
    "                                              for node in nx.from_numpy_array(thresholded_data[\"Off\"][:, :, i, j]).nodes ] \n",
    "                                              for i in range(num_subjects)])\n",
    "    centrality_net_local_eff_On = np.array([[local_efficiency_node(nx.from_numpy_array(thresholded_data[\"On\"][:, :, i, j]), node) \n",
    "                                              for node in nx.from_numpy_array(thresholded_data[\"On\"][:, :, i, j]).nodes] \n",
    "                                              for i in range(num_subjects)])\n",
    "    centrality_measures_Off[\"local_eff\"][j][:][:] = centrality_net_local_eff_Off / local_eff_null_Off[:, np.newaxis]\n",
    "    centrality_measures_On[\"local_eff\"][j][:][:] = centrality_net_local_eff_On / local_eff_null_On[:, np.newaxis]                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Weighted Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b99b22bc7d42d5ae3077291aaae80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Computing the global network measures for weighted graph\n",
    "# for now not computing any null hypothesis metrics as done in Skidmore 2011\n",
    "for j in tqdm(range(num_thresholds_wei_test)): \n",
    "    \n",
    "    # normalized characteristic path and global efficiency\n",
    "    charpath_network_Off, global_eff_network_Off = charpath_and_global_eff_wei(thresholded_data_dist[\"Off\"][:, :, :, j], num_subjects)\n",
    "    charpath_null_Off, global_eff_null_Off = charpath_and_global_eff(null_data_dist[\"Off\"][:, :, :, j], num_subjects)\n",
    "    charpath_network_On, global_eff_network_On = charpath_and_global_eff(thresholded_data_dist[\"On\"][:, :, :, j], num_subjects)\n",
    "    charpath_null_On, global_eff_null_On = charpath_and_global_eff(null_data_dist[\"On\"][:, :, :, j], num_subjects)\n",
    "\n",
    "    network_measures_wei_Off[\"norm_charact_path_length\"][j][:] = charpath_network_Off / charpath_null_Off\n",
    "    network_measures_wei_On[\"norm_charact_path_length\"][j][:] = charpath_network_On / charpath_null_On\n",
    "    network_measures_wei_Off[\"norm_global_eff\"][j][:] = global_eff_network_Off / global_eff_null_Off\n",
    "    network_measures_wei_On[\"norm_global_eff\"][j][:]= global_eff_network_On / global_eff_null_On\n",
    "    \n",
    "    # normalized clustering coefficient\n",
    "    network_clustering_coef_Off = np.array([np.mean(bct.clustering_coef_wu(thresholded_data_dist[\"Off\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    null_clustering_coef_Off = np.array([np.mean(bct.clustering_coef_bu(null_data_dist[\"Off\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    network_clustering_coef_On = np.array([np.mean(bct.clustering_coef_wu(thresholded_data_dist[\"On\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    null_clustering_coef_On = np.array([np.mean(bct.clustering_coef_bu(null_data_dist[\"On\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    network_measures_wei_Off[\"norm_clustering_coef\"][j][:] = network_clustering_coef_Off / null_clustering_coef_Off\n",
    "    network_measures_wei_On[\"norm_clustering_coef\"][j][:] = network_clustering_coef_On / null_clustering_coef_On\n",
    "    \n",
    "    # small-worldness\n",
    "    network_measures_wei_Off[\"small_worldness\"][j][:] = network_measures_wei_Off[\"norm_clustering_coef\"][j] / network_measures_wei_Off[\"norm_charact_path_length\"][j]\n",
    "    network_measures_wei_On[\"small_worldness\"][j][:] = network_measures_wei_On[\"norm_clustering_coef\"][j] / network_measures_wei_On[\"norm_charact_path_length\"][j] \n",
    "    \n",
    "    # Size of the Largest Connected Component (LCC)\n",
    "    network_measures_wei_Off[\"size_of_LCC\"][j][:] = np.array([len(max(nx.connected_components(nx.from_numpy_array(thresholded_data_dist[\"Off\"][:, :, i, j])))) for i in range(num_subjects)])\n",
    "    network_measures_wei_On[\"size_of_LCC\"][j][:] = np.array([len(max(nx.connected_components(nx.from_numpy_array(thresholded_data_dist[\"On\"][:, :, i, j])))) for i in range(num_subjects)])\n",
    "    \n",
    "    # Clustering coefficient saved for all nodes\n",
    "    centrality_net_clust_Off = np.array([bct.clustering_coef_wu(thresholded_data_dist[\"Off\"][:, :, i, j]) for i in range(num_subjects)])\n",
    "    centrality_net_clust_On = np.array([bct.clustering_coef_wu(thresholded_data_dist[\"On\"][:, :, i, j]) for i in range(num_subjects)])\n",
    "    # circumventing the issue of having a 0 clustering coefficient for a given region by dividing by the mean clustering coefficent (over all regions) \n",
    "    # of the null-hypothesis graph for a given graph density and subject. \n",
    "    centrality_measures_wei_Off[\"clustering_coef\"][j][:][:] = centrality_net_clust_Off #/ null_clustering_coef_Off[:, np.newaxis]\n",
    "    centrality_measures_wei_On[\"clustering_coef\"][j][:][:] = centrality_net_clust_On #/ null_clustering_coef_On[:, np.newaxis]\n",
    "    \n",
    "    # Degree for all nodes\n",
    "    centrality_net_deg_Off = np.array([bct.degrees_und(thresholded_data_dist[\"Off\"][:, :, i, j]) for i in range(num_subjects)])\n",
    "    centrality_net_deg_On = np.array([bct.degrees_und(thresholded_data_dist[\"On\"][:, :, i, j]) for i in range(num_subjects)])\n",
    "    null_mean_deg_Off = np.array([np.mean(bct.degrees_und(null_data_dist[\"Off\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    null_mean_deg_On = np.array([np.mean(bct.degrees_und(null_data_dist[\"On\"][:, :, i, j])) for i in range(num_subjects)])\n",
    "    centrality_measures_wei_Off[\"degree\"][j][:][:] = centrality_net_deg_Off / null_mean_deg_Off[:, np.newaxis]\n",
    "    centrality_measures_wei_On[\"degree\"][j][:][:] = centrality_net_deg_On / null_mean_deg_On[:, np.newaxis]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15576/1315002387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_thresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Local Efficiency (weighted networks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnetwork_measures_wei_Off\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"local_eff\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_eff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholded_data_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Off\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_subjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnetwork_measures_wei_Off\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"local_eff\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_eff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholded_data_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"On\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_subjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for j in tqdm(range(num_thresholds)): \n",
    "    # Local Efficiency (weighted networks)\n",
    "    network_measures_wei_Off[\"local_eff\"][j][:][:] = local_eff(thresholded_data_dist[\"Off\"][:, :, :, j], num_subjects)\n",
    "    network_measures_wei_Off[\"local_eff\"][j][:][:] = local_eff(thresholded_data_dist[\"On\"][:, :, :, j], num_subjects)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for j in tqdm(range(num_thresholds)): \n",
    "    # Local efficiency for all nodes (weighted networks)\n",
    "    centrality_net_local_eff_wei_Off = np.array([[local_efficiency_node(nx.from_numpy_array(thresholded_data_dist[\"Off\"][:, :, i, j]), node) \n",
    "                                              for node in nx.from_numpy_array(thresholded_data_dist[\"Off\"][:, :, i, j]).nodes ] \n",
    "                                              for i in range(num_subjects)])\n",
    "    centrality_net_local_eff_wei_On = np.array([[local_efficiency_node(nx.from_numpy_array(thresholded_data_dist[\"On\"][:, :, i, j]), node) \n",
    "                                              for node in nx.from_numpy_array(thresholded_data_dist[\"On\"][:, :, i, j]).nodes] \n",
    "                                              for i in range(num_subjects)])\n",
    "    centrality_measures_wei_Off[\"local_eff\"][j][:][:] = centrality_net_local_eff_Off #/ local_eff_null_Off[:, np.newaxis]\n",
    "    centrality_measures_wei_On[\"local_eff\"][j][:][:] = centrality_net_local_eff_On #/ local_eff_null_On[:, np.newaxis]  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Saving and Loading network measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Saving network measures to .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert numpy arrays to lists to make them JSON serializable\n",
    "for key, value in network_measures_Off.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        network_measures_Off[key] = value.tolist()\n",
    "for key, value in network_measures_On.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        network_measures_On[key] = value.tolist()\n",
    "for key, value in centrality_measures_Off.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        centrality_measures_Off[key] = value.tolist()\n",
    "for key, value in centrality_measures_On.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        centrality_measures_On[key] = value.tolist()\n",
    "\n",
    "# dump the dictionary in a new file\n",
    "with open('network_measures_Off.txt', 'w') as f:\n",
    "    f.write(json.dumps(network_measures_Off, indent=4))\n",
    "with open('network_measures_On.txt', 'w') as f:\n",
    "    f.write(json.dumps(network_measures_On, indent=4))\n",
    "with open('centrality_measures_Off.txt', 'w') as f:\n",
    "    f.write(json.dumps(centrality_measures_Off, indent=4))\n",
    "with open('centrality_measures_On.txt', 'w') as f:\n",
    "    f.write(json.dumps(centrality_measures_On, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "\n",
    "# Same thing for weighted networks\n",
    "for key, value in network_measures_wei_Off.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        network_measures_wei_Off[key] = value.tolist()\n",
    "for key, value in network_measures_wei_On.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        network_measures_wei_On[key] = value.tolist()\n",
    "for key, value in centrality_measures_wei_Off.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        centrality_measures_wei_Off[key] = value.tolist()\n",
    "for key, value in centrality_measures_wei_On.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        centrality_measures_wei_On[key] = value.tolist()\n",
    "\n",
    "with open('network_measures_wei_Off.txt', 'w') as f:\n",
    "    f.write(json.dumps(network_measures_wei_Off, indent=4))\n",
    "with open('network_measures_wei_On.txt', 'w') as f:\n",
    "    f.write(json.dumps(network_measures_wei_On, indent=4))\n",
    "with open('centrality_measures_wei_Off.txt', 'w') as f:\n",
    "    f.write(json.dumps(centrality_measures_wei_Off, indent=4))\n",
    "with open('centrality_measures_wei_On.txt', 'w') as f:\n",
    "    f.write(json.dumps(centrality_measures_wei_On, indent=4))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Loading global network measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the data back from the file\n",
    "with open('network_measures_Off.txt', 'r') as f:\n",
    "    network_measures_Off = json.load(f)\n",
    "with open('network_measures_On.txt', 'r') as f:\n",
    "    network_measures_On = json.load(f)\n",
    "with open('centrality_measures_Off.txt', 'r') as f:\n",
    "    centrality_measures_Off = json.load(f)\n",
    "with open('centrality_measures_On.txt', 'r') as f:\n",
    "    centrality_measures_On = json.load(f)\n",
    "\n",
    "# Convert lists back to numpy arrays\n",
    "for key, value in network_measures_Off.items():\n",
    "    network_measures_Off[key] = np.array(value)\n",
    "for key, value in network_measures_On.items():\n",
    "    network_measures_On[key] = np.array(value)  \n",
    "for key, value in centrality_measures_Off.items():\n",
    "    centrality_measures_Off[key] = np.array(value)  \n",
    "for key, value in centrality_measures_On.items():\n",
    "    centrality_measures_On[key] = np.array(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'centrality_measures_wei_Off.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50192/3674730554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'network_measures_wei_On.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnetwork_measures_wei_On\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'centrality_measures_wei_Off.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcentrality_measures_wei_Off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'centrality_measures_wei_On.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'centrality_measures_wei_Off.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "\n",
    "# Same thing for weighted networks\n",
    "with open('network_measures_wei_Off.txt', 'r') as f:\n",
    "    network_measures_wei_Off = json.load(f)\n",
    "with open('network_measures_wei_On.txt', 'r') as f:\n",
    "    network_measures_wei_On = json.load(f)\n",
    "with open('centrality_measures_wei_Off.txt', 'r') as f:\n",
    "    centrality_measures_wei_Off = json.load(f)\n",
    "with open('centrality_measures_wei_On.txt', 'r') as f:\n",
    "    centrality_measures_wei_On = json.load(f)\n",
    "\n",
    "for key, value in network_measures_wei_Off.items():\n",
    "    network_measures_wei_Off[key] = np.array(value)\n",
    "for key, value in network_measures_wei_On.items():\n",
    "    network_measures_wei_On[key] = np.array(value)  \n",
    "for key, value in centrality_measures_wei_Off.items():\n",
    "    centrality_measures_wei_Off[key] = np.array(value)  \n",
    "for key, value in centrality_measures_wei_On.items():\n",
    "    centrality_measures_wei_On[key] = np.array(value)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 104)\n",
      "(5, 104)\n",
      "(9, 104)\n",
      "(5, 104)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(network_measures_Off[\"norm_charact_path_length\"].shape)\n",
    "print(network_measures_wei_Off[\"norm_charact_path_length\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri_env_clust",
   "language": "python",
   "name": "fmri_env_clust"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
